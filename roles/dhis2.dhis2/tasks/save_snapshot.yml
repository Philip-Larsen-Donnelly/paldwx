---
- file:
    path: "{{ storage }}/tmp"
    owner: "{{ postgresql_user }}"
    group: "{{ postgresql_user }}"
    recurse: yes
    state: "directory"

- name: Clean up one-time jobs that never started
  postgresql_query:
    db: "{{ instance.db_name }}"
    port: "{{ postgresql_port_map[instance.postgresql_version] | default(omit) }}"
    query: delete from jobconfiguration where lastexecutedstatus = 'NOT_STARTED' and schedulingtype = 'ONCE_ASAP';
  become: true
  become_user: "{{ postgresql_user }}"
  vars:
    ansible_ssh_pipelining: true

# download the demo db (if not empty param)
- name: get the current date time
  set_fact: db_snap_date="{{lookup('pipe','date \"+%Y-%m-%d_%H%M%S\"')}}"

- name: Dump database snapshot (without analytics) to a file (with compression)
  postgresql_db:
    name: "{{ instance.db_name }}"
    port: "{{ postgresql_port_map[instance.postgresql_version] | default(omit) }}"
    state: dump
    target: "{{ storage }}/tmp/{{ db_snap_date }}_{{ instance.name }}.{{ db_dump_format }}"
    target_opts: "{{ dump_cluster }} {{ dump_options }}"
  become: true
  become_user: "{{ postgresql_user }}"
  vars:
    ansible_ssh_pipelining: true
    dump_options: "{{ '-Fc -Z 9  --no-owner --no-privileges -T analytics* -T _*' if (db_dump_format == 'pgc') else ' --no-owner --no-privileges -T analytics* -T _*' }}"
    dump_cluster: "--cluster {{instance.postgresql_version | default(postgresql_version)}}/main"
  # not sure I need the "when" above - it might prevent snapshotting an instance started from empty.

# - set_fact: save_suffix="{{ db_save_suffix | default('') }}"

- name: copy the snapshot to s3
  aws_s3:
    aws_access_key: "{{ aws_access_key }}"
    aws_secret_key: "{{ aws_secret_key }}"
    bucket: dhis2-database-backups
    object: "{{ ansible_host }}/{{ instance_name }}/{{ db_snap_date }}_{{ instance.name }}{{ save_suffix }}.{{ db_dump_format }}"
    src: "{{ storage }}/tmp/{{ db_snap_date }}_{{ instance.name }}.{{ db_dump_format }}"
    mode: put
    metadata:
      saved_by: "{{ awx_user_name | default('unknown') }}"
      comment: "{{ db_snapshot_comment | default('none') }}"
      original_source: "{{ instance.db_demo }}"
      war_file: "{{ instance.war_file }}"
  vars:
    save_suffix: "{{ db_save_suffix | default('') }}"


- name: Create a tar.gz archive of the instance filestore.
  block:

  - name: create the archive
    community.general.archive:
      path: "{{ storage }}/instances/{{ instance.name }}/home/dhis2-{{ instance.db_name|replace('_','-')  }}-store/*"
      dest: "{{ storage }}/tmp/{{ db_snap_date }}_{{ instance.name }}.filestore"
      format: gz
      force_archive: true
    ignore_errors: yes

  - name: commit the filestore to s3
    aws_s3:
      aws_access_key: "{{ aws_access_key }}"
      aws_secret_key: "{{ aws_secret_key }}"
      bucket: dhis2-database-backups
      object: "{{ ansible_host }}/{{ instance_name }}/{{ db_snap_date }}_{{ instance.name }}{{ save_suffix }}.filestore"
      src: "{{ storage }}/tmp/{{ db_snap_date }}_{{ instance.name }}.filestore"
      mode: put
      metadata:
        saved_by: "{{ awx_user_name | default('unknown') }}"
        comment: "{{ db_commit_comment | default('none') }}"
        original_source: "{{ instance.db_demo }}"
        war_file: "{{ instance.war_file }}"
    vars:
      save_suffix: "{{ db_save_suffix | default('') }}"
    ignore_errors: yes

  when: dhis2_filestore.type | default('filesystem') == 'filesystem'

- name: remove local dump files
  file:
    path: "{{ item }}"
    state: absent
  with_items:
    - "{{ storage }}/tmp/{{ db_snap_date }}_{{ instance.name }}.{{ db_dump_format }}"
    - "{{ storage }}/tmp/{{ db_snap_date }}_{{ instance.name }}.filestore"
